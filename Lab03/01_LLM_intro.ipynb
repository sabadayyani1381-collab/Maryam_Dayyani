{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf12054",
   "metadata": {},
   "source": [
    "# üåü LLM\n",
    "## What is an LLM?\n",
    "\n",
    "**LLM** stands for **Large Language Model**.\n",
    "It is a type of artificial intelligence trained on huge amounts of text so it can understand and generate human-like language.\n",
    "\n",
    "‚úîÔ∏è Think of an LLM as:\n",
    "\n",
    "A super-advanced text-prediction machine.\n",
    "Similar to how your phone predicts the next word when you type, but thousands of times more powerful.\n",
    "\n",
    "‚úîÔ∏è What can an LLM do?\n",
    "\n",
    "- Answer questions\n",
    "- Summarize text\n",
    "- Translate languages\n",
    "- Write code\n",
    "- Have conversations\n",
    "- Help with homework or explanations\n",
    "\n",
    "‚úîÔ∏è Why ‚Äúlarge‚Äù?\n",
    "\n",
    "Because it uses billions of parameters (mathematical patterns learned from text). The more parameters, usually the more capable the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff1fbc",
   "metadata": {},
   "source": [
    "# What is a Neural Network?\n",
    "\n",
    "A neural network is a computer model inspired by the human brain.\n",
    "\n",
    "‚úîÔ∏è Basic idea:\n",
    "\n",
    "- The brain has **neurons** that pass signals to each other.\n",
    "- A neural network has artificial neurons, which are just **math** functions.\n",
    "- Each connection has a **weight** (a number the network learns).\n",
    "\n",
    "‚úîÔ∏è Goal:\n",
    "\n",
    "Neural networks learn patterns.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Show pictures of cats ‚Üí it learns to recognize cats\n",
    "- Show a lot of English sentences ‚Üí it learns grammar patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbf715",
   "metadata": {},
   "source": [
    "# Early Neural Networks\n",
    "\n",
    "Early neural networks were small and could only solve simple problems.\n",
    "\n",
    "Types included:\n",
    "\n",
    "- Perceptrons (**1950s**): basic binary classifiers\n",
    "- Feedforward networks (1980s‚Äì1990s)\n",
    "- Convolutional Neural Networks (CNNs) for images\n",
    "- Recurrent Neural Networks (RNNs) for sequences\n",
    "- ...\n",
    "\n",
    "They worked‚Ä¶ but not very well for long text.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/perceptron.png\" width=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023877e",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "Deep learning ‚Üí Neural networks with **many layers** (‚Äúdeep‚Äù networks)\n",
    "A deep learning model learns by adjusting millions or billions of internal numbers called **weights**.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/deep_network.png\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "\n",
    "## Why the word \"deep\"?\n",
    "Because the neural network has many layers stacked on top of each other:\n",
    "Input ‚Üí Layer 1 ‚Üí Layer 2 ‚Üí Layer 3 ‚Üí ‚Ä¶ ‚Üí Layer N ‚Üí Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9ecae",
   "metadata": {},
   "source": [
    "# ‚ö° 1. The Breakthrough: Transformers (2017)\n",
    "In 2017, **Google** published a paper called:\n",
    "\n",
    "**\"Attention is All You Need\"**\n",
    "\n",
    "This introduced the Transformer architecture.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer.png\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "\n",
    "‚úîÔ∏è What made Transformers special?\n",
    "\n",
    "Attention mechanism.\n",
    "\n",
    "It lets the model:\n",
    "\n",
    "- Look at all words at once\n",
    "- Decide which words are important\n",
    "- Remember long-range relationships\n",
    "\n",
    "Example:\n",
    "\n",
    "In the sentence\n",
    "‚ÄúThe cat that chased the dog was hungry.‚Äù\n",
    "The model can link ‚Äúcat‚Äù ‚Üî ‚Äúwas hungry‚Äù, even though they are far apart.\n",
    "This solved the ‚Äúforgetting‚Äù problem.\n",
    "\n",
    "‚úîÔ∏è Why Transformers enabled huge models:\n",
    "\n",
    "- They can process text in parallel\n",
    "- They scale extremely well with more data\n",
    "- They learn long-range meaning and structure\n",
    "\n",
    "This architecture is the foundation of all modern LLMs:\n",
    "- GPT series (OpenAI)\n",
    "- Llama (Meta)\n",
    "- Gemini (Google)\n",
    "- Claude (Anthropic)\n",
    "- Mistral models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e581b",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è 2. GPU?\n",
    "\n",
    "A **GPU (Graphics Processing Unit)** is a special type of processor originally designed for one job:\n",
    "\n",
    "üëâ Render graphics for video games.\n",
    "\n",
    "To do this, a GPU needs to perform millions of small math operations in parallel (at the same time).\n",
    "This ‚Äúparallel processing‚Äù is EXACTLY what neural networks need.\n",
    "\n",
    "Neural networks rely heavily on:\n",
    "\n",
    "- matrix multiplication\n",
    "- vector operations\n",
    "- linear algebra\n",
    "\n",
    "GPUs accelerate these operations thousands of times faster than CPUs.\n",
    "\n",
    "Before around 2010, GPUs were mostly for gaming.\n",
    "\n",
    "Then a breakthrough happened:\n",
    "- Nvidia created CUDA (2006), allowing GPUs to run general-purpose math\n",
    "- Researchers discovered GPUs dramatically speed up neural networks\n",
    "- Deep learning started exploding\n",
    "\n",
    "Without GPUs:\n",
    "- Training an LLM would take hundreds of years\n",
    "- Transforming billions of parameters would be impossible\n",
    "\n",
    "Today‚Äôs massive GPU clusters (thousands of GPUs working together) make LLM training feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd891f",
   "metadata": {},
   "source": [
    "# üåê 3. Huge Amounts of Data\n",
    "\n",
    "Early neural networks had:\n",
    "\n",
    "- tiny datasets\n",
    "- low-quality text sources\n",
    "- limited access to the internet\n",
    "\n",
    "To train an LLM you need massive amounts of text, such as:\n",
    "\n",
    "- billions of web pages\n",
    "- books\n",
    "- Wikipedia\n",
    "- forums\n",
    "- code repositories\n",
    "\n",
    "This scale of data did not exist or was not easily accessible 20 years ago.\n",
    "\n",
    "Thanks to the internet age, researchers can now build datasets containing trillions of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190bcbe",
   "metadata": {},
   "source": [
    "### üöÄ When these three things came together, LLMs became possible\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/chatGPT.png\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "A **generative pre-trained transformer (GPT)** is a type of **large language model (LLM)** that is widely used in generative AI chatbots.GPTs are based on a **deep learning** architecture called the **transformer**. They are **pre-trained** on large datasets of unlabeled content, and able to generate novel content.\n",
    "\n",
    "Parameters:\n",
    "- A perceptron with 2 inputs -> 3 parameters\n",
    "- GPT-3 -> 175 billion parameters\n",
    "- GPT-5 -> 1.7‚Äì1.8 trillion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909732a1",
   "metadata": {},
   "source": [
    "## üß† Hallucination?\n",
    "\n",
    "In the context of Large Language Models (LLMs) like ChatGPT:\n",
    "\n",
    "Hallucination means the model makes up information that is false, inaccurate, or completely fabricated, but presents it as if it were true.\n",
    "\n",
    "‚úîÔ∏è Examples:\n",
    "\n",
    "- Fictional facts:\n",
    "\n",
    "User: ‚ÄúWho won the Nobel Prize in 2020 for physics?‚Äù\n",
    "Model: ‚ÄúIt was Dr. John Smith.‚Äù (Incorrect)\n",
    "\n",
    "- Made-up citations:\n",
    "\n",
    "User: ‚ÄúGive me a reference for this topic.‚Äù\n",
    "Model: ‚ÄúDoe, J. (2019). Advanced AI Studies. Journal of AI.‚Äù (Doesn‚Äôt exist)\n",
    "\n",
    "- Confident but wrong reasoning:\n",
    "\n",
    "User: ‚ÄúExplain how unicorns fly.‚Äù\n",
    "Model: Provides a plausible-sounding explanation even though unicorns aren‚Äôt real.\n",
    "\n",
    "## ‚öôÔ∏è Why Do LLMs Hallucinate?\n",
    "\n",
    "LLMs like GPT are not **‚Äúthinking machines‚Äù** ‚Äî they are pattern predictors. They generate text based on statistical patterns learned from data.\n",
    "\n",
    "Key reasons:\n",
    "### 1Ô∏è‚É£ They predict the most likely next word\n",
    "\n",
    "LLMs are trained to continue text plausibly, not necessarily correctly.\n",
    "They don‚Äôt have a true understanding of facts‚Äîthey just generate what sounds right.\n",
    "\n",
    "Analogy:\n",
    "Imagine a very smart autocomplete keyboard‚Äîit will suggest the most probable next word, even if it‚Äôs wrong.\n",
    "\n",
    "### 2Ô∏è‚É£ Training data is imperfect\n",
    "\n",
    "LLMs learn from text on the internet, books, code, articles‚Ä¶\n",
    "Some of that data is wrong, biased, or fictional.\n",
    "The model can reproduce errors from the data.\n",
    "\n",
    "### 3Ô∏è‚É£ Lack of real-world grounding\n",
    "\n",
    "LLMs don‚Äôt access real-time data (unless connected to a knowledge base or plugin).\n",
    "They can‚Äôt check facts on their own‚Äîthey rely on patterns they learned during training.\n",
    "\n",
    "### 4Ô∏è‚É£ Ambiguity in the prompt\n",
    "\n",
    "If the user asks vague or creative questions, the model may generate plausible-sounding but incorrect answers.\n",
    "\n",
    "Example: ‚ÄúExplain how humans breathe underwater‚Äù ‚Üí generates imaginative explanation, because it tries to be helpful even if impossible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028e2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
