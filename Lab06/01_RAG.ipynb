{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbce20f3",
   "metadata": {},
   "source": [
    "# **RAG (Retrieval-Augmented Generation) Systems**\n",
    "\n",
    "##  **1. What is a RAG System?**\n",
    "\n",
    "- A *Retrieval-Augmented Generation (RAG)* system is a type of AI system that combines large language models (LLMs) with external information sources (like documents, databases, or knowledge bases). Instead of relying purely on the knowledge baked into the LLM, a RAG system retrieves relevant information from external sources and uses it to generate more accurate, up-to-date, and contextually relevant responses.\n",
    "\n",
    "**Why do we need RAG?**\n",
    "\n",
    "LLMs have limitations:\n",
    "\n",
    "- Knowledge cutoff: They might not know recent events.\n",
    "- Hallucinations: They sometimes generate false or misleading information.\n",
    "- Limited memory: They can only “remember” so much context within a conversation.\n",
    "\n",
    "RAG allows LLMs to access external knowledge, improving:\n",
    "\n",
    "- Accuracy\n",
    "- Relevance\n",
    "- Coverage of specialized domains\n",
    "\n",
    "Example use cases:\n",
    "\n",
    "- Answering questions from large documents\n",
    "- Customer support using internal knowledge bases\n",
    "- Generating reports with current data\n",
    "\n",
    "**Why not just use an LLM alone?**\n",
    "\n",
    "- LLMs are trained on fixed datasets. If you ask about a niche domain (like proprietary company manuals or specific coding guidelines), the LLM may not know the answer.\n",
    "- RAG allows dynamic, up-to-date information retrieval, combining the strengths of search engines and LLMs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b146d568",
   "metadata": {},
   "source": [
    "##  **2. Components of a RAG System**\n",
    "\n",
    "A typical RAG system consists of the following components:\n",
    "\n",
    "- **Knowledge Base:**\n",
    "Where your source information is stored. Could be PDFs, text files, or a database.\n",
    "\n",
    "- **Embeddings Generator:**\n",
    "Converts documents and queries into numerical vectors that the machine can process.\n",
    "\n",
    "- **Vector Database (Vector DB):**\n",
    "Stores embeddings and allows fast similarity searches.\n",
    "\n",
    "- **Retriever:**\n",
    "Finds the most relevant documents for a given query by comparing vectors.\n",
    "\n",
    "- **Large Language Model (LLM):**\n",
    "Generates the final response using both the retrieved documents and the original query.\n",
    "\n",
    "- **Indexer / Chunker:**\n",
    "Prepares the documents for efficient storage and retrieval by splitting them into manageable pieces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc79ab2",
   "metadata": {},
   "source": [
    "##  **3. Vector Embeddings**\n",
    "\n",
    "**What are embeddings?**\n",
    "\n",
    "Embeddings are numerical representations of text in a high-dimensional space.\n",
    "Similar texts have similar embeddings (close in vector space), while different texts are far apart.\n",
    "\n",
    "**Why are embeddings used?**\n",
    "\n",
    "LLMs cannot directly search millions of documents efficiently.\n",
    "Embeddings let us measure semantic similarity:\n",
    "“How similar is this document to my query?”\n",
    "Done via distance metrics like cosine similarity.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/embeddings.png\" width=\"500\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2049a796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "aval_api_key=os.getenv(\"AVALAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(    \n",
    "    api_key=aval_api_key,\n",
    "    base_url=\"https://api.avalai.ir/v1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057aac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = client.embeddings.create(\n",
    "    input=\"Python is a programming language.\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "print(len(res1.data[0].embedding))\n",
    "print(res1.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = client.embeddings.create(\n",
    "    input=\"Bananas are yellow fruits.\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "print(len(res2.data[0].embedding))\n",
    "print(res2.data[0].embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3506db",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [res1.data[0].embedding, res2.data[0].embedding]\n",
    "# Convert to numpy arrays for similarity calculations\n",
    "vecs = [np.array(e) for e in embeddings]\n",
    "\n",
    "# Example: cosine similarity\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "similarity = cosine_similarity(vecs[0], vecs[1])\n",
    "print(similarity) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6636c58",
   "metadata": {},
   "source": [
    "##  **4. Vector Databases**\n",
    "\n",
    "**What is a Vector DB?**\n",
    "\n",
    "- A vector database is a storage system optimized for fast similarity search of embeddings.\n",
    "- Instead of searching full text, we search vectors to find the most semantically relevant documents.\n",
    "\n",
    "**Why use them?**\n",
    "\n",
    "- Efficient for large datasets (millions of documents)\n",
    "- Fast retrieval with approximate nearest neighbor (ANN) methods\n",
    "- Simplifies RAG pipelines\n",
    "\n",
    "**Popular Vector DBs:**\n",
    "\n",
    "- ChromaDB: Lightweight, Python-friendly, ideal for small/medium projects.\n",
    "- FAISS (Facebook AI Similarity Search): Optimized for large datasets, extremely fast.\n",
    "- Pinecone, Weaviate, Milvus: Other options with cloud support.\n",
    "\n",
    "```\n",
    "pip install chromadb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23d8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Use an embedding function\n",
    "embedding_fn = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=aval_api_key,\n",
    "    api_base=\"https://api.avalai.ir/v1\",\n",
    "    model_name=\"text-embedding-3-small\",\n",
    "    )\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"my_collection\",\n",
    "    embedding_function=embedding_fn)\n",
    "\n",
    "# Add documents\n",
    "collection.add(\n",
    "    documents=[\"Python is fun\", \"Bananas are yellow\"],\n",
    "    metadatas=[{\"source\": \"doc1\"}, {\"source\": \"doc2\"}],\n",
    "    ids=[\"1\", \"2\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5142da56",
   "metadata": {},
   "source": [
    "##  **5. Chunking and Indexing**\n",
    "\n",
    "**Chunking:**\n",
    "\n",
    "- Large documents are broken into smaller pieces (chunks) so the retriever can handle them efficiently.\n",
    "- Typical chunk sizes: 200–500 words.\n",
    "- Helps LLM focus on relevant pieces instead of overwhelming it with a huge document.\n",
    "\n",
    "**Indexing:**\n",
    "\n",
    "- Organizes chunks in the vector database.\n",
    "- Associates each chunk with its embedding and metadata (source, location).\n",
    "- Makes retrieval fast and precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40fc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=100):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8358c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_data = chunk_text(\"\"\"A big text:\n",
    "##  **3. Vector Embeddings**\n",
    "\n",
    "**What are embeddings?**\n",
    "\n",
    "Embeddings are numerical representations of text in a high-dimensional space.\n",
    "Similar texts have similar embeddings (close in vector space), while different texts are far apart.\n",
    "\n",
    "**Why are embeddings used?**\n",
    "\n",
    "LLMs cannot directly search millions of documents efficiently.\n",
    "Embeddings let us measure semantic similarity:\n",
    "“How similar is this document to my query?”\n",
    "Done via distance metrics like cosine similarity.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/embeddings.png\" width=\"500\">\n",
    "</p>\n",
    "##  **4. Vector Databases**\n",
    "\n",
    "**What is a Vector DB?**\n",
    "\n",
    "- A vector database is a storage system optimized for fast similarity search of embeddings.\n",
    "- Instead of searching full text, we search vectors to find the most semantically relevant documents.\n",
    "\n",
    "**Why use them?**\n",
    "\n",
    "- Efficient for large datasets (millions of documents)\n",
    "- Fast retrieval with approximate nearest neighbor (ANN) methods\n",
    "- Simplifies RAG pipelines\n",
    "\n",
    "**Popular Vector DBs:**\n",
    "\n",
    "- ChromaDB: Lightweight, Python-friendly, ideal for small/medium projects.\n",
    "- FAISS (Facebook AI Similarity Search): Optimized for large datasets, extremely fast.\n",
    "- Pinecone, Weaviate, Milvus: Other options with cloud support.\n",
    "\n",
    "```\n",
    "pip install chromadb\n",
    "```\n",
    "**5. Chunking and Indexing**\n",
    "\n",
    "**Chunking:**\n",
    "\n",
    "- Large documents are broken into smaller pieces (chunks) so the retriever can handle them efficiently.\n",
    "- Typical chunk sizes: 200–500 words.\n",
    "- Helps LLM focus on relevant pieces instead of overwhelming it with a huge document.\n",
    "\n",
    "**Indexing:**\n",
    "\n",
    "- Organizes chunks in the vector database.\n",
    "- Associates each chunk with its embedding and metadata (source, location).\n",
    "- Makes retrieval fast and precise.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb615b13",
   "metadata": {},
   "source": [
    "##  **6. Retriever**\n",
    "\n",
    "**What is a retriever?**\n",
    "- A retriever is the component that searches the vector DB to find the most relevant chunks for a query.\n",
    "\n",
    "**Types of retrievers:**\n",
    "- Dense retrievers: Use embeddings for similarity search (most common in RAG)\n",
    "- Sparse retrievers: Traditional keyword-based search (e.g., Elasticsearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is Python?\"\n",
    "results = collection.query(query_texts=[query], n_results=1)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484dacaa",
   "metadata": {},
   "source": [
    "### ✅ Key Takeaways:\n",
    "\n",
    "RAG systems combine retrieval with generation for more accurate, domain-specific responses.\n",
    "\n",
    "- Core components: embeddings, vector DB, retriever, chunking, LLM.\n",
    "- Vector databases make large-scale retrieval efficient.\n",
    "- Chunking and indexing are crucial for performance.\n",
    "-Building a RAG system involves embedding your documents, storing them in a vector DB, retrieving relevant chunks, and feeding them to an LLM for generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
